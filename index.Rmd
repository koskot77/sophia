---
title       : "\"Some\" NSG analysis"
subtitle    : 
author      : Khristian Kotov
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
github:
  user: kkotov 
  repo: talks
url:
  lib:    ../libraries
  assets: ../assets
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
ext_widgets : {rCharts: [libraries/nvd3, libraries/polycharts]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

## Introduction

While looking at job announcements on LinkedIn, I found one particularly attractive
opening for Algorithm Developer position at one company specializing in Next Generation
Sequencing (NGS) analysis. I emailed the contact person and received a task to with a
following description:

"Please do some analysis on the file [testReads.fastq.gz](http://dl.dropbox.com/u/68829208/testReads.fastq.gz)
and write a small report on your findings.

Notes:
 * This file contains NGS reads for some human DNA regions.
 * The fastq file format is described at http://en.wikipedia.org/wiki/FASTQ_format,
the Phred quality score is in sanger format not illumina.
 * The report should be in PDF format and figures rather than descriptive words is preferred.
Please also make clear description on your figures.
 * The report could be anything that you find interesting on the file. For example,
it could be patterns you found on the data, comments on the quality of the data or the variants
detected on this data, etc."

--- .class #id



## Input

The input file contains 84205 four-line records; one example record is shown below:

<hr>

<pre>
@HKVCQHG01APQJG rank=0000016 x=176.0 y=10.0 length=108
CTCGCGTGTCAAGACTCGGCAGCATCTCCACCTTAAGATGAGCTCTAATTTGTTGTTATGTGGCTCCTGTAAGTATAGTGGAGAACAGTGACGATCGCGACACGCGAG
+
IIIIIIIIIHEEEHIG;<8;DIIIIG>43;//1111?9IIIIII?;66111?13?88??I???DDFFIH?<<>DIIIFDBBFB==@EIIIIIIIIIIIIIIIIIIIIF
</pre>

<hr>

The second line contains a sequence made of letters $A$, $C$, $G$, $T$, and sometimes $N$

The forth line encodes [Phred+33](http://en.wikipedia.org/wiki/Phred_quality_score) quality of a measurement at each position as follows:
 * convert a symbol to an ASCII code (e.g. 'I' $\rightarrow$ 73)
 * subtract 33 as ASCII alphabet starts at code 33 (e.g. Q['I'] = 73 - 33 = 40)
 * now probability of an error in a specific position is then given by $~p = 10^{-Q/10}$

Letter $N$ appears only in 432 records at positions with 0 quality

--- .class #id


## Problems

Neither I performed an NGS analysis before nor I had an idea what I am expected to do

So I came up with a few simple checks without understanding how practical they are:

 * count number of unique patters of a certain length in the data
 * scan all sequences and identify patterns that repeat more often than others
 * group similar reads together and find how many clusters they form


For these checks I need:

 * efficient engine to search patterns in the data
  * avoid string comparisons in favor of hashing
 * fast machinery that compares the sequences and measures similarity
  * comparison of all-to-all reads takes $\mathcal{O}(N^2) \sim \mathcal{O}(10^{10})$ iterations! 
 * algorithm to merge similar sequences into clusters

--- .class #id


## Toolbox

I coded up all of the necessary algorithms in several C++ files:

 * Hashing machinery is implemented in [toolbox.h](https://github.com/koskot77/sophia/blob/master/toolbox.h) file
  * <span style="color:blue;">sequence2number</span> function converts a short (<32 symbols) sequence into a number
  * <span style="color:blue;">NumericSequence</span> class represents sequence as an array of numbers
  * <span style="color:blue;">LookUpTable</span> hash table is initialized with patterns sought; provides "find" functionality

 * Needleman-Wunsch sequence alignment algorithm implemented in [alignment.cc](https://github.com/koskot77/sophia/blob/master/alignment.cc) file
  * <span style="color:blue;">alignmentScoreMatrix</span> computes the score for the two aligning sequences
  * <span style="color:blue;">reconstruction</span> inserts gaps based on the score and achieves the best match

 * Clustering algorithm implemented in [cluster.cc](https://github.com/koskot77/sophia/blob/master/cluster.cc) file for a stand-alone executable
  * reads text file formatted as "read #, read #, distance\n"
  * merges reads into clusters until distance between reads is greater than some cutoff

--- .class #id

## Workflow

Pattern frequency test is implemented in [analysis.cc](https://github.com/koskot77/sophia/blob/master/analysis.cc) file:
 * slides over the sequences [letter by letter](https://github.com/koskot77/sophia/blob/master/analysis.cc#L122:L133) and
[counts](https://github.com/koskot77/sophia/blob/master/analysis.cc#L126:L127) encountered patterns

Search for similar reads is implemented in [overlap.cc](https://github.com/koskot77/sophia/blob/master/overlaps.cc) file:
 * <span style="color:blue;">findMatches</span> preprocessing step runs for all $\mathcal{O}(N^2)$ pair combinations:
  * [chops a reference read](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L65:L69) into search patterns ($k$mers)
  * checks all other reads and remember those [firing any of the search patterns](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L96)
 * <span style="color:blue;">lookCloser</span> step calculates distance defined as a penalty score of an alignment:
  * runs [full-blown sequence alignment](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L142:L148) for pairs from the preprocessing step
  * ignores gaps inserted into the two aligned sequences at the [beginning and end](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L150:L152)
  * [in the common "overlap" region increment the score by 1 for a mismatch or gap](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L156)

Clustering the reads is completed executing `./cluster output.csv CUTOFF`

--- .class #id

## Analysis: data variability

To warm up, let's count number of unique patterns of a certain length $L$ found in data:

<div align="center">
```{r nvd3plot, results = 'asis', comment = NA, message = F, echo = F, fig.align='center'}

#if( !file.exists("sophia/") ){
#   system("git clone https://github.com/koskot77/sophia.git")
#   system("cd sophia && ln -s ../testReads.fastq && g++ -Wall -o test analysis.cc && for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ; do time ./test $i ; mv output.csv output_$i.csv ; done")
#}
#combinations <- data.frame( width=integer(), found=integer() )
#for(w in 1:32){
#  print(w)
#  an<-read.csv(file=paste('sophia/output_',w,'.csv',sep=""),header=T,sep=',')
#  combinations <- rbind( combinations, data.frame( width=w, found=log2(dim(an)[[1]]) ) )
#}

combinations <- data.frame( width=seq(1,32,1), found=c(
2, 4, 6, 8, 10, 12, 13.99, 15.89, 17.36, 18.24, 18.71, 18.97,
19.13, 19.25, 19.36, 19.46, 19.55, 19.63, 19.72, 19.79, 19.87, 19.94,
20.01, 20.07, 20.14, 20.20, 20.26, 20.32, 20.38, 20.43, 20.48, 20.50) )

#require(ggplot2)
#ggplot(data=combinations,aes(x=width,y=found)) +geom_point() +labs(x="pattern length (symbols)",y="log2(# of unique patterns)",title="Data variability")

require(rCharts)

p1 <- rPlot(found ~ width, data = combinations, type = 'point')
p1$set(width  = 850)
p1$set(height = 350)
p1$set(title = "log2(# of unique patterns)")
p1$guides(
  x = list(
    min = 0.5,
    max = 32.5,
    numticks = length( combinations$width ),
#    labels = combinations$width,
    title = "pattern length (symbols)"
  ),
  y = list(
    min = 0,
    max = 22
  )
)
p1$show('inline')
```
</div>

Number of unique combinations of the 4 letters grows as $4^L$ until $L$~10 where it slows


--- .class #id


## Analysis: pattern frequency test

Examining output files from the previous step with something like this:

<span style="bold;font-size:smaller;color:brown">
`sed -e 's|,| |g' *.csv | awk 'BEGIN{m=0;p=""} $1!~/ID/{if(m<$3){m=$3;p=$2}} END{print p}'`
</span>

allows us to identify the most frequent relatively short patterns as follows:

length |             sequence               | frequency |
-------|------------------------------------|-----------|
20     | `---------AAGACTCGGCAGCATCTCCA---` |    47K    |
21     | `--------GAAGACTCGGCAGCATCTCCA---` |    24K    |
26     | `TCAGACACGAAGACTCGGCAGCATCT------` |    12K    |
30     | `TCAGACACGAAGACTCGGCAGCATCTCCAT--` |    3.8K   |
32     | `CTTCCATTGACCACATCTCCTCTGACTTCAAA` |    2.2K   |

<br>

I.e. ~2.6% of the reads have the `CTTCCATTGACCACATCTCCTCTGACTTCAAA` pattern

--- .class #id


## Analysis: grouping similar records

```{r results = 'asis', comment = NA, message = F, echo = F}
if( !file.exists("clusters9.csv") ){
    system("g++ -g -Wall -o cluster cluster.cc && ./cluster w.csv 9 && mv clusters.csv clusters9.csv"); 
}
cl <- read.csv(file='clusters9.csv',header=T,sep=',')
```

Grouping reads with up to 9 mismatches/gaps apart (edit distance) gives `r length( unique(cl$read1) )` clusters

Cluster density diminishes slowly from center to periphery:

<div align="center">
```{r results = 'asis', comment = NA, message = F, echo = F}

p2 <- rPlot(x="bin(distance,1)", y="count(distance)", data=cl, type="bar")

p2$set(width  = 850)
p2$set(height = 300)
p2$set(title = "Cluster profile (summed up for all clusters)")
p2$guides(
  x = list(
    min = 0,
    max = 11,
    numticks = 11,
    title = "Edit distance"
  ),
  y = list(
    title = ""
  )
)
p2$show('inline')
```
</div>

```{r results = 'asis', comment = NA, message = F, echo = F}
if( !file.exists("clusters8.csv") ){
    system("g++ -g -Wall -o cluster cluster.cc && ./cluster w.csv 8 && mv clusters.csv clusters8.csv"); 
}
cl <- read.csv(file='clusters8.csv',header=T,sep=',')
```

Excess in bin #10 suggests that clustering with such cutoff merges some fine-scale structure

Using 8 for the cutoff removes the excess and results in `r length( unique(cl$read1) )` clusters

--- .class #id


## What was I really expected to demonstrate?

I failed the test because I did not identify there were several samples mixed in a single file

Wet labs sequence multiple samples in one run [tagging each sample with a "barcode"](http://res.illumina.com/documents/products/illumina_sequencing_introduction.pdf)

De-multiplexing such data can be achieved by clustering the reads with similar beginnings

Grouping sequences with 10 consecutive matches within first 15 symbols ([barcodes2.cc](https://github.com/koskot77/sophia/blob/master/barcodes2.cc)) yields:

cluster |      barcode      | frequency |
--------|-------------------|-----------|
1       | `.{0,2}TCGCGTGTC` |   19.3K   |
2       | `.{0,2}TATCGCGAG` |   20.7K   |
3       | `.{0,2}GTGTCTCTA` |   21.4K   |
4       | `.{0,2}TCAGACACG` |   21.9K   |

(`.{0,2}` pattern allows any 0, 1, or 2 symbols; usually it is 'C' for #1,3 and 'A' for #2,4)

Only about 0.5K reads are left out of these 4 major clusters

<!-- [A simple cross-check in Galaxy](https://usegalaxy.org/u/koskot77/w/sophia1-1) with the Barcode Splitter tool confirms all these findings -->

--- .class #id

## What these data tell us?

Aligning the reads to human genome reveals a targeted sequencing of these genes:

chromosome  | gene  |     id    |         description          |
------------|-------|-----------|------------------------------|
chr17       | BRCA1 | NM_027676 | Breast cancer 1, early onset |
chr13       | BRCA2 | NM_000059 | Breast cancer 2, early onset |

<br>

[A simple workflow](https://usegalaxy.org/u/koskot77/w/sophia3) finds [25](hg19vars.vcf) ([48](hg38vars.vcf)) high quality variants wrt. hg19 (hg38) reference genome

Some of these can be found in dbSNP database:
[rs3752451](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=3752451),
[rs1801406](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=1801406),
[rs206075](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=206075),
[rs206076](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=206076),
[rs206080](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=206080),
[rs169547](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=169547),
[rs9534262](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=9534262),
[rs3092994](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=3092994),
[rs1799966](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=1799966),
[rs1060915](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=1060915),
[rs1799949](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=1799949)

<!-- rs206076 and rs206080 are not in #3 and rs3092994 is not in #4 -->

The four barcodes identify four different individuals (check homo/heterozygosity of the variants)

--- .class #id

## Summary

<br>

I am happy I had a chance to play with some decent size datasets
and prove that I know how important efficiency is. I also learned
how to analyze these data.

Nonetheless, some points in the evaluation are still a mystery for me:
"The data is generated with certain enrichment technology and certain sequencers."
If only I knew what are the implications of using different sequencers and why would
that be important.

I did my best to jump on NGS data analysis, but I learned that little things
that I am not aware of can make a big difference. Although, I see learning such
things is only a question of time for me.

<!--
Analysed reads: 
```{r, engine="bash", comment = NA, message = F, echo = F}
expr `wc testReads.fastq | awk '{print $1}'` / 4
```

Average read size:
```{r, engine="bash", comment = NA, message = F, echo = F}
 perl -ane '$l++;if($l==2){print length($F[0])."\n";}if($l==4){$l=0;}' testReads.fastq  > l.csv
```
```{r, comment = NA, message = F, echo = F}
l <- read.csv(file="l.csv",header=F,sep=',')
paste( round( mean(l$V1), digits=3), " +- ", round( sd(l$V1), digits=3) )
```

All the reads group in `r length( unique( cl$read1 ) )` clusters 
-->

<br>

