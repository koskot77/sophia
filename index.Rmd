---
title       : "\"Some\" NGS data analysis"
subtitle    : 
author      : Khristian Kotov
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
github:
  user: kkotov 
  repo: talks
url:
  lib:    ../libraries
  assets: ../assets
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
ext_widgets : {rCharts: [libraries/nvd3, libraries/polycharts]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

## Introduction

While looking at job announcements on LinkedIn, I found one particularly attractive
opening for Algorithm Developer position at one company specializing in Next Generation
Sequencing (NGS) analysis. I emailed the contact person and received a task to with a
following description:

"Please do some analysis on the file [testReads.fastq.gz](http://dl.dropbox.com/u/68829208/testReads.fastq.gz)
and write a small report on your findings.

Notes:
 * This file contains NGS reads for some human DNA regions.
 * The fastq file format is described at http://en.wikipedia.org/wiki/FASTQ_format,
the Phred quality score is in sanger format not illumina.
 * The report should be in PDF format and figures rather than descriptive words is preferred.
Please also make clear description on your figures.
 * The report could be anything that you find interesting on the file. For example,
it could be patterns you found on the data, comments on the quality of the data or the variants
detected on this data, etc."

--- .class #id



## Input

The input file contains 84205 four-line records; one example record is shown below:

<hr>

<pre>
@HKVCQHG01APQJG rank=0000016 x=176.0 y=10.0 length=108
CTCGCGTGTCAAGACTCGGCAGCATCTCCACCTTAAGATGAGCTCTAATTTGTTGTTATGTGGCTCCTGTAAGTATAGTGGAGAACAGTGACGATCGCGACACGCGAG
+
IIIIIIIIIHEEEHIG;<8;DIIIIG>43;//1111?9IIIIII?;66111?13?88??I???DDFFIH?<<>DIIIFDBBFB==@EIIIIIIIIIIIIIIIIIIIIF
</pre>

<hr>

The second line contains a sequence made of letters $A$, $C$, $G$, $T$, and sometimes $N$

The forth line encodes [Phred](http://en.wikipedia.org/wiki/Phred_quality_score) quality of a measurement at each position as follows:
 * convert a symbol to an ASCII code (e.g. 'I' $\rightarrow$ 73)
 * subtract 33 as ASCII alphabet starts at code 33 (e.g. Q['I'] = 73 - 33 = 40)
 * now probability of an error in a specific position is then given by $~p = 10^{-Q/10}$

Letter $N$ appears only in 432 records at positions with 0 quality

--- .class #id


## Problems

Neither I performed an NGS analysis before nor I had an idea what I am expected to do

So I came up with a few simple checks without understanding how practical they are:

 * count number of unique patters of a certain length in the data
 * scan all sequences and identify patterns that repeat more often than others
 * group similar reads together and find how many clusters they form


For these checks I need:

 * efficient engine to search patterns in the data
  * avoid string comparisons in favor of hashing
 * fast machinery that compares the sequences and measures similarity
  * comparison of all-to-all reads takes $O(N^2) \sim O(10^{10})$ iterations! 
 * algorithm to merge similar sequences into clusters

--- .class #id


## Toolbox

I coded up all of the necessary algorithms in several C++ files:

 * Hashing machinery is implemented in [toolbox.h](https://github.com/koskot77/sophia/blob/master/toolbox.h) file
  * <span style="color:blue;">sequence2number</span> function converts a short (<32 symbols) sequence into a number
  * <span style="color:blue;">NumericSequence</span> class represents sequence as an array of numbers
  * <span style="color:blue;">LookUpTable</span> hash table is initialized with patterns sought; provides "find" functionality

 * Needleman-Wunsch sequence alignment algorithm implemented in [alignment.cc](https://github.com/koskot77/sophia/blob/master/alignment.cc) file
  * <span style="color:blue;">alignmentScoreMatrix</span> computes the score for the two aligning sequences
  * <span style="color:blue;">reconstruction</span> inserts gaps based on the score and achieves the best match

 * Clustering algorithm implemented in [cluster.cc](https://github.com/koskot77/sophia/blob/master/cluster.cc) file for a stand-alone executable
  * reads text file formatted as "read #, read #, distance\n"
  * merges reads into clusters until distance between reads is greater than some cutoff

--- .class #id

## Workflow

Pattern frequency test is implemented in [analysis.cc](https://github.com/koskot77/sophia/blob/master/analysis.cc) file:
 * slides over the sequences [letter by letter](https://github.com/koskot77/sophia/blob/master/analysis.cc#L122:L133) and
[counts](https://github.com/koskot77/sophia/blob/master/analysis.cc#L126:L127) encountered patterns

Search for similar reads is implemented in [overlap.cc](https://github.com/koskot77/sophia/blob/master/overlaps.cc) file:
 * <span style="color:blue;">findMatches</span> preprocessing step runs for all $O(N^2)$ pair combinations:
  * [chops a reference read](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L65:L69) into search patterns 
  * checks all other reads and remember those [firing any of the search patterns](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L96)
 * <span style="color:blue;">lookCloser</span> step calculates distance defined as a penalty score of an alignment:
  * runs [full-blown sequence alignment](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L142:L148) for pairs from the preprocessing step
  * ignores gaps inserted into the two aligned sequences at the [beginning and end](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L150:L152)
  * [in the common "overlap" region increment the score by 1 for a mismatch or gap](https://github.com/koskot77/sophia/blob/master/overlaps.cc#L156)

Clustering the reads is completed executing `./cluster output.csv CUTOFF`

--- .class #id

## Analysis: data variability

To warm up, let's count number of unique patterns of a certain length $L$ found in data:

<center>
```{r nvd3plot, results = 'asis', comment = NA, message = F, echo = F}

#if( !file.exists("sophia/") ){
#   system("git clone https://github.com/koskot77/sophia.git")
#   system("cd sophia && ln -s ../testReads.fastq && g++ -Wall -o test analysis.cc && for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ; do time ./test $i ; mv output.csv output_$i.csv ; done")
#}
#combinations <- data.frame( width=integer(), found=integer() )
#for(w in 1:32){
#  print(w)
#  an<-read.csv(file=paste('sophia/output_',w,'.csv',sep=""),header=T,sep=',')
#  combinations <- rbind( combinations, data.frame( width=w, found=log2(dim(an)[[1]]) ) )
#}

combinations <- data.frame( width=seq(1,32,1), found=c(
2, 4, 6, 8, 10, 12, 13.99, 15.89, 17.36, 18.24, 18.71, 18.97,
19.13, 19.25, 19.36, 19.46, 19.55, 19.63, 19.72, 19.79, 19.87, 19.94,
20.01, 20.07, 20.14, 20.20, 20.26, 20.32, 20.38, 20.43, 20.48, 20.50) )

#require(ggplot2)
#ggplot(data=combinations,aes(x=width,y=found)) +geom_point() +labs(x="pattern length (symbols)",y="log2(# of unique patterns)",title="Data variability")

require(rCharts)

p1 <- rPlot(found ~ width, data = combinations, type = 'point')
p1$set(width  = 850)
p1$set(height = 350)
p1$set(title = "log2(# of unique patterns)")
p1$guides(
  x = list(
    min = 0.5,
    max = 32.5,
    numticks = length( combinations$width ),
#    labels = combinations$width,
    title = "pattern length (symbols)"
  ),
  y = list(
    min = 0,
    max = 22
  )
)
p1$show('inline')
```
</center>

Number of unique combinations of the 4 letters grows as $4^L$ until $L$~10 where it slows


--- .class #id


## Analysis: pattern frequency test

Examining output files from the previous step with something like this:

<span style="bold;font-size:smaller;color:brown">
`sed -e 's|,| |g' *.csv | awk 'BEGIN{m=0;p=""} $1!~/ID/{if(m<$3){m=$3;p=$2}} END{print p}'`
</span>

allows us to identify the most frequent relatively short patterns as follows:

length |             sequence               | frequency |
-------|------------------------------------|-----------|
20     | `---------AAGACTCGGCAGCATCTCCA---` |    47K    |
21     | `--------GAAGACTCGGCAGCATCTCCA---` |    24K    |
26     | `TCAGACACGAAGACTCGGCAGCATCT------` |    12K    |
30     | `TCAGACACGAAGACTCGGCAGCATCTCCAT--` |    3.8K   |
32     | `CTTCCATTGACCACATCTCCTCTGACTTCAAA` |    2.2K   |

<br>

I.e. ~2.6% of the reads have the `CTTCCATTGACCACATCTCCTCTGACTTCAAA` pattern

--- .class #id


## Analysis: grouping similar records

```{r results = 'asis', comment = NA, message = F, echo = F}
if( !file.exists("clusters9.csv") ){
    system("g++ -g -Wall -o cluster cluster.cc && ./cluster w.csv 9 && mv clusters.csv clusters9.csv"); 
}
cl <- read.csv(file='clusters9.csv',header=T,sep=',')
```

Grouping reads with max "distance" of 9 mismatches and gaps gives `r length( unique(cl$read1) )` clusters

Cluster density diminishes slowly from center to periphery:

<center>
```{r results = 'asis', comment = NA, message = F, echo = F}

p2 <- rPlot(x="bin(distance,1)", y="count(distance)", data=cl, type="bar")

p2$set(width  = 850)
p2$set(height = 300)
p2$set(title = "Cluster profile (summed up for all clusters)")
p2$guides(
  x = list(
    min = 0,
    max = 11,
    numticks = 11,
    title = "distance from center"
  ),
  y = list(
    title = ""
  )
)
p2$show('inline')
```
</center>

```{r results = 'asis', comment = NA, message = F, echo = F}
if( !file.exists("clusters8.csv") ){
    system("g++ -g -Wall -o cluster cluster.cc && ./cluster w.csv 8 && mv clusters.csv clusters8.csv"); 
}
cl <- read.csv(file='clusters8.csv',header=T,sep=',')
```

Excess in bin #10 suggests that clustering with such cutoff merges some fine-scale structure

Using 8 for the cutoff removes the excess and results in `r length( unique(cl$read1) )` clusters

--- .class #id


## Summary

<br>

Running time of my algorithms allows to perform this analysis on a modest laptop

Further optimization is also possible

<!--
Analysed reads: 
```{r, engine="bash", comment = NA, message = F, echo = F}
expr `wc testReads.fastq | awk '{print $1}'` / 4
```

Average read size:
```{r, engine="bash", comment = NA, message = F, echo = F}
 perl -ane '$l++;if($l==2){print length($F[0])."\n";}if($l==4){$l=0;}' testReads.fastq  > l.csv
```
```{r, comment = NA, message = F, echo = F}
l <- read.csv(file="l.csv",header=F,sep=',')
paste( round( mean(l$V1), digits=3), " +- ", round( sd(l$V1), digits=3) )
```

All the reads group in `r length( unique( cl$read1 ) )` clusters 
-->

<br>
Obviously, twisting the data in various ways cannot replace a tiny bit of knowledge
of what are the stories that the data can possibly tell us. I did my best to jump on
NGS data analysis, but I only learned that I need to learn more ...
